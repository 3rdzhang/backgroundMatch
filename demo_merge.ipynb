{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d521fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import  os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b8cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(img_dir, image_processor, model, max_len = 10):\n",
    "    feats = {}\n",
    "    files = os.listdir(img_dir)\n",
    "    \n",
    "    for f in files[:max_len]:\n",
    "        f_path = os.path.join(img_dir,f)\n",
    "        image = Image.open(f_path)\n",
    "        print(\"extracting feats of {}\".format(f_path))\n",
    "        inputs = image_processor(image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            last_hidden_states = outputs.last_hidden_state[:,0]\n",
    "            # feats.append(last_hidden_states)\n",
    "            feats[f] = last_hidden_states\n",
    "        print(\"success extracting feats of {}\".format(f_path))\n",
    "    return feats\n",
    "\n",
    "def get_result(ori_feats, back_feats, topn):\n",
    "    result = {}\n",
    "    for ori_f in ori_feats.keys():\n",
    "        dist = {}\n",
    "        for back_f in back_feats.keys():\n",
    "            d = torch.nn.functional.cosine_similarity(ori_feats[ori_f], back_feats[back_f])\n",
    "            # d = torch.ao.ns.fx.utils.compute_cosine_similarity(ori_feats[ori_f], back_feats[back_f])\n",
    "            dist[back_f] = d\n",
    "        dist = sorted(dist.items(), key=lambda d:d[1], reverse = True)\n",
    "        result[ori_f] = dist[:topn]\n",
    "    return result  \n",
    "\n",
    "def combine_result(back_result, profile_result):\n",
    "    for ori in back_result.keys():\n",
    "        back_result[ori].append(profile_result[ori])\n",
    "    return back_result\n",
    "\n",
    "def show_result(back_result, profile_result,  ori_path, back_path, profile_path):\n",
    "    plt.figure()\n",
    "    row_num = len(back_result)\n",
    "    fig, axs = plt.subplots(row_num, 5, figsize=(25, 15))\n",
    "    i = 1\n",
    "    for r in back_result.keys():\n",
    "        image = Image.open(os.path.join(ori_path, r))\n",
    "        plt.subplot(row_num, 5, i)\n",
    "        i = i + 1\n",
    "        plt.imshow(image)\n",
    "        for rr in back_result[r]:\n",
    "            image = Image.open(os.path.join(back_path, rr[0]))\n",
    "            plt.subplot(row_num, 5, i)\n",
    "            i = i + 1\n",
    "            plt.imshow(image)\n",
    "         \n",
    "        image = Image.open(os.path.join(profile_path, profile_result[r][0][0]))\n",
    "        plt.subplot(row_num, 5, i)\n",
    "        i = i + 1\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_path = \"./测试2/原图_small\"\n",
    "back_path=\"./测试2/场景图/正面\"\n",
    "profile_path=\"./测试2/场景图/侧面\"\n",
    "\n",
    "ori_feats = extract_feature(ori_path, image_processor, model, max_len = 100)\n",
    "with open(\"./ori_feats\", 'wb') as f:\n",
    "    pickle.dump(ori_feats, f)\n",
    "    \n",
    "back_feats = extract_feature(back_path, image_processor, model, max_len = 100)\n",
    "with open(\"./back_feats\", 'wb') as f:\n",
    "    pickle.dump(back_feats, f)\n",
    "    \n",
    "profile_feats = extract_feature(profile_path, image_processor, model, max_len = 100)\n",
    "import pickle\n",
    "with open(\"./profile_feats\", 'wb') as f:\n",
    "    pickle.dump(profile_feats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8935ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ori_feats\", 'rb') as f:\n",
    "    ori_feats = pickle.load(f)\n",
    "\n",
    "with open(\"./back_feats\", 'rb') as f:\n",
    "    back_feats = pickle.load(f)\n",
    "    \n",
    "with open(\"./profile_feats\", 'rb') as f:\n",
    "    profile_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1249a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(ori_feats, back_feats, data_path):\n",
    "    data = []\n",
    "    for ori in ori_feats:\n",
    "        for back in back_feats:\n",
    "            label = torch.randint(0,2,(1,))\n",
    "            d = torch.cat([ori_feats[ori][0], back_feats[back][0], label])\n",
    "            data.append(d)\n",
    "    ret = torch.stack(data, 0)\n",
    "    np.savetxt(data_path,ret.numpy(),fmt='%.2f',delimiter=',')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40ba762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    " \n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c80c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, data_path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(data_path, header=None)\n",
    "        # store the inputs and outputs        \n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    " \n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.hidden4 = Linear(2, 1)\n",
    "        xavier_uniform_(self.hidden4.weight)\n",
    "        self.act3 = Sigmoid()\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        left = X[:,:768]\n",
    "        right = X[:,768:]\n",
    "        sim = torch.nn.functional.cosine_similarity(left, right)\n",
    "        sim = torch.reshape(sim,(-1,1))\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "#         print(X.shape)\n",
    "#         print(sim.shape)\n",
    "        X = torch.cat([X, sim], -1)\n",
    "#         print(X.shape)\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316e86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    " \n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    " \n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    " \n",
    "# make a class prediction for one row of data\n",
    "def predict(ori_feat, back_feats, model):\n",
    "    ret = []\n",
    "    for back in back_feats:\n",
    "        tmp = torch.cat([ori_feat[0], back[0]], -1)\n",
    "        ret.append(tmp)\n",
    "    pred_data = torch.stack(ret)\n",
    "    print(pred_data.shape)\n",
    "    # make prediction\n",
    "    yhat = model(pred_data)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91889792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./ori_feats\", 'rb') as f:\n",
    "#     ori_feats = pickle.load(f)\n",
    "\n",
    "# with open(\"./back_feats\", 'rb') as f:\n",
    "#     back_feats = pickle.load(f)\n",
    "    \n",
    "# with open(\"./profile_feats\", 'rb') as f:\n",
    "#     profile_feats = pickle.load(f)\n",
    "train_data_path =  \"./embd_data.csv\"\n",
    "train_data = gen_data(ori_feats, back_feats, data_path = train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20e0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "585177e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.480\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "train_dl, test_dl = prepare_data(train_data_path)\n",
    "n_inputs = 1536\n",
    "model = MLP(n_inputs)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab43097",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./model.pt\" \n",
    "torch.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4236181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden1): Linear(in_features=1536, out_features=10, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (hidden2): Linear(in_features=10, out_features=8, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (hidden3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (hidden4): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (act3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_model = torch.load(MODEL_PATH)\n",
    "r_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e624f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_feats(img_name, feats_dict):\n",
    "    return {img_name: feats_dict[img_name]}\n",
    "\n",
    "def get_group_feats(img_dir, feats_dict):\n",
    "    files = os.listdir(img_dir)\n",
    "    feats = { k : feats_dict[k] for k in files}\n",
    "    return feats\n",
    "\n",
    "def cal_score(img_feats, back_feats, topN = 3): \n",
    "    ori_img = list(img_feats.keys())[0]\n",
    "    ori_feas = list(img_feats.values())[0]\n",
    "    back = []\n",
    "    key = []\n",
    "    for k in back_feats:\n",
    "        back.append(f_back[k])\n",
    "        key.append(k)\n",
    "    y = predict(ori_feas, back, model)\n",
    "    pred_result = {k:v.detach().numpy()[0] for k,v in zip(key, y)}\n",
    "    ret = sorted(pred_result.items(), key=lambda d:d[1], reverse = True)\n",
    "    return {ori_img : ret[:topN]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be5a3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ori = get_one_feats(\"1.png\", ori_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfae23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_back = get_group_feats(\"./测试2/场景图/正面\", back_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be03913a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 1536])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1.png': [('客厅装饰画空白样机 (26).jpg', 0.68778807),\n",
       "  ('客厅装饰画空白样机 (9).jpg', 0.6114947),\n",
       "  ('客厅装饰画空白样机 (45).jpg', 0.60454434)]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_score(f_ori, f_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d105f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
